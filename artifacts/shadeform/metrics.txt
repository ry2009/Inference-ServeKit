# HELP python_gc_objects_collected_total Objects collected during gc
# TYPE python_gc_objects_collected_total counter
python_gc_objects_collected_total{generation="0"} 1888.0
python_gc_objects_collected_total{generation="1"} 321.0
python_gc_objects_collected_total{generation="2"} 10.0
# HELP python_gc_objects_uncollectable_total Uncollectable objects found during GC
# TYPE python_gc_objects_uncollectable_total counter
python_gc_objects_uncollectable_total{generation="0"} 0.0
python_gc_objects_uncollectable_total{generation="1"} 0.0
python_gc_objects_uncollectable_total{generation="2"} 0.0
# HELP python_gc_collections_total Number of times this generation was collected
# TYPE python_gc_collections_total counter
python_gc_collections_total{generation="0"} 141.0
python_gc_collections_total{generation="1"} 12.0
python_gc_collections_total{generation="2"} 1.0
# HELP python_info Python platform information
# TYPE python_info gauge
python_info{implementation="CPython",major="3",minor="10",patchlevel="12",version="3.10.12"} 1.0
# HELP process_virtual_memory_bytes Virtual memory size in bytes.
# TYPE process_virtual_memory_bytes gauge
process_virtual_memory_bytes 1.736634368e+09
# HELP process_resident_memory_bytes Resident memory size in bytes.
# TYPE process_resident_memory_bytes gauge
process_resident_memory_bytes 5.971968e+07
# HELP process_start_time_seconds Start time of the process since unix epoch in seconds.
# TYPE process_start_time_seconds gauge
process_start_time_seconds 1.75877885432e+09
# HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.
# TYPE process_cpu_seconds_total counter
process_cpu_seconds_total 1.5299999999999998
# HELP process_open_fds Number of open file descriptors.
# TYPE process_open_fds gauge
process_open_fds 16.0
# HELP process_max_fds Maximum number of open file descriptors.
# TYPE process_max_fds gauge
process_max_fds 1024.0
# HELP primerl_tokens_total Tokens generated
# TYPE primerl_tokens_total counter
primerl_tokens_total{model="llama3-8b-instruct",phase="prefill"} 0.0
primerl_tokens_total{model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",phase="prefill"} 0.0
primerl_tokens_total{model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",phase="decode"} 126.0
# HELP primerl_tokens_created Tokens generated
# TYPE primerl_tokens_created gauge
primerl_tokens_created{model="llama3-8b-instruct",phase="prefill"} 1.758778983088339e+09
primerl_tokens_created{model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",phase="prefill"} 1.7587790017330616e+09
primerl_tokens_created{model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",phase="decode"} 1.758779001785696e+09
# HELP primerl_request_latency_seconds Request latency by route/model
# TYPE primerl_request_latency_seconds histogram
primerl_request_latency_seconds_bucket{le="0.005",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="Step"} 0.0
primerl_request_latency_seconds_bucket{le="0.01",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="Step"} 0.0
primerl_request_latency_seconds_bucket{le="0.025",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="Step"} 0.0
primerl_request_latency_seconds_bucket{le="0.05",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="Step"} 0.0
primerl_request_latency_seconds_bucket{le="0.075",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="Step"} 0.0
primerl_request_latency_seconds_bucket{le="0.1",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="Step"} 0.0
primerl_request_latency_seconds_bucket{le="0.25",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="Step"} 0.0
primerl_request_latency_seconds_bucket{le="0.5",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="Step"} 0.0
primerl_request_latency_seconds_bucket{le="0.75",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="Step"} 0.0
primerl_request_latency_seconds_bucket{le="1.0",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="Step"} 0.0
primerl_request_latency_seconds_bucket{le="2.5",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="Step"} 0.0
primerl_request_latency_seconds_bucket{le="5.0",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="Step"} 0.0
primerl_request_latency_seconds_bucket{le="7.5",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="Step"} 0.0
primerl_request_latency_seconds_bucket{le="10.0",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="Step"} 0.0
primerl_request_latency_seconds_bucket{le="+Inf",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="Step"} 126.0
primerl_request_latency_seconds_count{model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="Step"} 126.0
primerl_request_latency_seconds_sum{model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="Step"} 2.2160615595438196e+011
primerl_request_latency_seconds_bucket{le="0.005",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="EndEpisode"} 3.0
primerl_request_latency_seconds_bucket{le="0.01",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="EndEpisode"} 3.0
primerl_request_latency_seconds_bucket{le="0.025",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="EndEpisode"} 3.0
primerl_request_latency_seconds_bucket{le="0.05",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="EndEpisode"} 3.0
primerl_request_latency_seconds_bucket{le="0.075",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="EndEpisode"} 3.0
primerl_request_latency_seconds_bucket{le="0.1",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="EndEpisode"} 3.0
primerl_request_latency_seconds_bucket{le="0.25",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="EndEpisode"} 3.0
primerl_request_latency_seconds_bucket{le="0.5",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="EndEpisode"} 3.0
primerl_request_latency_seconds_bucket{le="0.75",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="EndEpisode"} 3.0
primerl_request_latency_seconds_bucket{le="1.0",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="EndEpisode"} 3.0
primerl_request_latency_seconds_bucket{le="2.5",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="EndEpisode"} 3.0
primerl_request_latency_seconds_bucket{le="5.0",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="EndEpisode"} 3.0
primerl_request_latency_seconds_bucket{le="7.5",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="EndEpisode"} 3.0
primerl_request_latency_seconds_bucket{le="10.0",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="EndEpisode"} 3.0
primerl_request_latency_seconds_bucket{le="+Inf",model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="EndEpisode"} 3.0
primerl_request_latency_seconds_count{model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="EndEpisode"} 3.0
primerl_request_latency_seconds_sum{model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="EndEpisode"} 0.0
# HELP primerl_request_latency_seconds_created Request latency by route/model
# TYPE primerl_request_latency_seconds_created gauge
primerl_request_latency_seconds_created{model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="Step"} 1.7587790017857172e+09
primerl_request_latency_seconds_created{model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",route="EndEpisode"} 1.7587790021262267e+09
# HELP primerl_queue_depth Requests queued
# TYPE primerl_queue_depth gauge
primerl_queue_depth{model="llama3-8b-instruct"} 0.0
primerl_queue_depth{model="TinyLlama/TinyLlama-1.1B-Chat-v1.0"} 0.0
# HELP primerl_prefix_cache_hits_total Prefix cache hits
# TYPE primerl_prefix_cache_hits_total counter
primerl_prefix_cache_hits_total{model="TinyLlama/TinyLlama-1.1B-Chat-v1.0"} 3.0
# HELP primerl_prefix_cache_hits_created Prefix cache hits
# TYPE primerl_prefix_cache_hits_created gauge
primerl_prefix_cache_hits_created{model="TinyLlama/TinyLlama-1.1B-Chat-v1.0"} 1.7587790017329006e+09
# HELP primerl_prefix_cache_misses_total Prefix cache misses
# TYPE primerl_prefix_cache_misses_total counter
primerl_prefix_cache_misses_total{model="llama3-8b-instruct"} 1.0
# HELP primerl_prefix_cache_misses_created Prefix cache misses
# TYPE primerl_prefix_cache_misses_created gauge
primerl_prefix_cache_misses_created{model="llama3-8b-instruct"} 1.7587789830881498e+09
# HELP primerl_kv_resident_bytes Resident KV bytes
# TYPE primerl_kv_resident_bytes gauge
primerl_kv_resident_bytes{model="TinyLlama/TinyLlama-1.1B-Chat-v1.0"} 0.0
